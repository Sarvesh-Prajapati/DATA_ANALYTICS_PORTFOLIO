{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **From Databricks Notebook (sourced as .py, converted to .ipynb)**"
      ],
      "metadata": {
        "id": "cn81pdsc9Cgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbutils.fs.ls('/FileStore/')  # peeking into '/FileStore/'"
      ],
      "metadata": {
        "id": "B247B193Pp1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbutils.fs.ls('/FileStore/tables/')   # peeking into '/FileStore/tables/'"
      ],
      "metadata": {
        "id": "NZq0pfqlPo-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Above o/p shows where our required file is residing. Copy that path and use it as shown in cell ahead:**"
      ],
      "metadata": {
        "id": "6FU3joPiPj81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('csv').option('inferSchema', True).option('header', True).load('/FileStore/tables/BigMart_Sales.csv')\n",
        "df.display()"
      ],
      "metadata": {
        "id": "MJCerNovPkba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Reading JSON**"
      ],
      "metadata": {
        "id": "HgFzJ5ybPdW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_json = spark.read.format('json').option('inferSchema', True)\\\n",
        "                    .option('header', True)\\\n",
        "                    .option('multiline', False)\\\n",
        "                    .load('/FileStore/tables/drivers.json')\n",
        "\n",
        "df_json.display()"
      ],
      "metadata": {
        "id": "R_zNrk84PdyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Schema Definition**"
      ],
      "metadata": {
        "id": "Jf-_JGwRPW6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "fsk3N4YgPYR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Creating a DDL Schema to apply to our frame 'df'**"
      ],
      "metadata": {
        "id": "b-_hlH-lPB6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_ddl_schema = '''\n",
        "                    Item_Identifier STRING,\n",
        "                    Item_Weight STRING,\n",
        "                    Item_Fat_Content STRING,\n",
        "                    Item_Visibility DOUBLE,\n",
        "                    Item_Type STRING,\n",
        "                    Item_MRP DOUBLE,\n",
        "                    Outlet_Identifier STRING,\n",
        "                    Outlet_Establishment_Year INT,\n",
        "                    Outlet_Size STRING,\n",
        "                    Outlet_Location_Type STRING,\n",
        "                    Outlet_Type STRING,\n",
        "                    Item_Outlet_Sales DOUBLE\n",
        "                '''\n",
        "\n",
        "df = spark.read.format('csv')\\\n",
        "                .schema(my_ddl_schema)\\\n",
        "                    .option('header', True)\\\n",
        "                        .load('/FileStore/tables/BigMart_Sales.csv')"
      ],
      "metadata": {
        "id": "ajgsxc1lPDd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.display()"
      ],
      "metadata": {
        "id": "VzyXpkfiO8Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()   # 'Item_Weight' was DOUBLE earlier, now it is STRING"
      ],
      "metadata": {
        "id": "mRAbdDQeO83L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Creating a StructType() Schema**"
      ],
      "metadata": {
        "id": "CkVcTk6oO2kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "a1Z_pyU7OzXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_struct_schema = StructType([\n",
        "                            StructField('Item_Identifier', StringType(), True),  # True --> NULLs allowed\n",
        "                            StructField('Item_Weight', StringType(), True),\n",
        "                            StructField('Item_Fat_Content', StringType(), True),\n",
        "                            StructField('Item_Visibility', StringType(), True),\n",
        "                            StructField('Item_Type', StringType(), True),\n",
        "                            StructField('Item_MRP', StringType(), True),\n",
        "                            StructField('Outlet_Identifier', StringType(), True),\n",
        "                            StructField('Outlet_Establishment_Year', StringType(), True),\n",
        "                            StructField('Outlet_Size', StringType(), True),\n",
        "                            StructField('Outlet_Location_Type', StringType(), True),\n",
        "                            StructField('Outlet_Type', StringType(), True),\n",
        "                            StructField('Item_Outlet_Sales', StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "xfQ2AA6aOyRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying above 'my_struct_schema' to our df\n",
        "\n",
        "df = spark.read.format('csv').schema(my_struct_schema).option('header', True).load('/FileStore/tables/BigMart_Sales.csv')\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "u1zWedRpOuO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **So that was about data loading, custom schema creation and application on df. Let's reload the CSV file for our further tasks.**"
      ],
      "metadata": {
        "id": "lQ0ithXqOlxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('csv').option('inferSchema', True).option('header', True).load('/FileStore/tables/BigMart_Sales.csv')\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "B3DwW6uPOmLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Now that we have the original file's content again, we proceed to SELECT.**"
      ],
      "metadata": {
        "id": "lT3k1v4FOhJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **SELECT**"
      ],
      "metadata": {
        "id": "qyub1pvtOXgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns  # glancing at all cols"
      ],
      "metadata": {
        "id": "WELcLXZPOX9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('Item_Identifier', 'Item_Weight', 'Item_Fat_Content').display()"
      ],
      "metadata": {
        "id": "M6YFKcoRObE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ANOTHER WAY to write above cell's code\n",
        "\n",
        "df.select(col('Item_Identifier'), col('Item_Weight'), col('Item_Fat_Content')).display()"
      ],
      "metadata": {
        "id": "_kxfkA3UOcL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ALIAS**"
      ],
      "metadata": {
        "id": "Gt4k4kBPOQ4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(col('Item_Identifier').alias('Item_ID')).display()"
      ],
      "metadata": {
        "id": "xzQZ-C4xORRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **FILTER**"
      ],
      "metadata": {
        "id": "DhZPv3WIOGq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario-1**"
      ],
      "metadata": {
        "id": "3yQnd83XOHG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col('Item_Fat_Content') == 'Regular').display()"
      ],
      "metadata": {
        "id": "tsbTfwGcOHyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 2**"
      ],
      "metadata": {
        "id": "HnaArWzjN-vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pay attention to the syntax\n",
        "\n",
        "df.filter((col('Item_Type') == 'Soft Drinks') & (col('Item_Weight') < 10)).display()"
      ],
      "metadata": {
        "id": "i_8zvLmUN_SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 3**"
      ],
      "metadata": {
        "id": "XTWTtZK-N5e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter((col('Outlet_Size').isNull()) & (col('Outlet_Location_Type').isin('Tier 1', 'Tier 2'))).display()"
      ],
      "metadata": {
        "id": "sY9XOq3CN58q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting only first 3 columns from above output  (just an exercise)\n",
        "\n",
        "df.filter((col('Outlet_Size').isNull()) & (col('Outlet_Location_Type').isin('Tier 1', 'Tier 2')))\\\n",
        "    .select(col('Item_Identifier'), col('Item_Weight'), col('Item_Fat_Content'))\\\n",
        "        .display()"
      ],
      "metadata": {
        "id": "ZFLK5_78N3UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **withColumnRenamed()** -- Changing column name at frame level"
      ],
      "metadata": {
        "id": "NvBFXzleNlsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumnRenamed('Item_Weight', 'Item_Wt').display()"
      ],
      "metadata": {
        "id": "JsWIqJ_QNmSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **withColumn()** -- Adding new column"
      ],
      "metadata": {
        "id": "PCPhmMjiNhE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 1**"
      ],
      "metadata": {
        "id": "CFDPKbCJNU5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New col's name is 'Flag' and it will contain the literal 'new' as values\n",
        "\n",
        "df = df.withColumn('Flag', lit(\"new\"))\n",
        "df.display()"
      ],
      "metadata": {
        "id": "92XrLLCkNVaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creating a new col based on some calculation"
      ],
      "metadata": {
        "id": "mXm4ghJfNOwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('Multiply', col('Item_Weight') * col('Item_MRP')).display()"
      ],
      "metadata": {
        "id": "vcFT_GfPNPN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 2**"
      ],
      "metadata": {
        "id": "cALTMPu_NJ2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a frame's col name as it is (e.g. 'Item_Fat_Content' below) modifies the existing col of that name\n",
        "# If a different col name is specified, then withColumn() creates a new col of that different name\n",
        "# regexp_replace() finds a string pattern and replaces it with new string\n",
        "\n",
        "df.withColumn('Item_Fat_Content', regexp_replace(col('Item_Fat_Content'), \"Regular\", \"Reg\"))\\\n",
        "    .withColumn('Item_Fat_Content', regexp_replace(col('Item_Fat_Content'), \"[Ll]ow [Ff]at\", \"LF\"))\\\n",
        "        .display()"
      ],
      "metadata": {
        "id": "Map5-wRHNBZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Type Casting**"
      ],
      "metadata": {
        "id": "L4aCp4IKM235"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('Item_Weight', col('Item_Weight').cast(StringType()))"
      ],
      "metadata": {
        "id": "f0vVuz7bM0wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "bnOs63l1Mzfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Sorting**"
      ],
      "metadata": {
        "id": "Sd7fh0K2MMtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 1**"
      ],
      "metadata": {
        "id": "l9YqEP_UMNrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort(col('Item_Weight').desc()).display()"
      ],
      "metadata": {
        "id": "ohbim6cJMS3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 2**"
      ],
      "metadata": {
        "id": "VnLRn6ntMYhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort(col('Item_Visibility').asc()).display()"
      ],
      "metadata": {
        "id": "sV5PTw2SMXw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 3** : Sorting on multiple columns\n",
        "\n"
      ],
      "metadata": {
        "id": "D6HZNLa4Mcet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort(['Item_Weight', 'Item_Visibility'], ascending = [0, 0])\\\n",
        "    .display()          # [0, 0] --> false, false"
      ],
      "metadata": {
        "id": "B0vevXBxMdst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 4** : Sorting on multiple columns (one asc, one desc)\n"
      ],
      "metadata": {
        "id": "okB8iGX1Me9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# O/p is 1st sorted desc on Item_Weight & then based on that, 'Item_Visibility' is sorted asc;\n",
        "# that's why, 0 is not coming up in 'Item_Visibility' at the top\n",
        "\n",
        "df.sort(['Item_Weight', 'Item_Visibility'], ascending = [0, 1])\\\n",
        "    .display()"
      ],
      "metadata": {
        "id": "y6B9Y4-JMfmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **LIMIT**"
      ],
      "metadata": {
        "id": "UVvCxqz8Mk6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.limit(5).display()"
      ],
      "metadata": {
        "id": "9yIiqrbkMmFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **INTERMEDIATE TRANSFORMATIONS**"
      ],
      "metadata": {
        "id": "dZ8FMFBeILnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **DROP**"
      ],
      "metadata": {
        "id": "VSsCpI3OIMby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('Item_Visibility').display()  # multi col drop --> df.drop('col_name', 'col_name', ...)"
      ],
      "metadata": {
        "id": "hdFEM-gzL3eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **DROP_DUPLICATES**"
      ],
      "metadata": {
        "id": "0MdLBQdlLsL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 1**"
      ],
      "metadata": {
        "id": "RIAdq5xtLsrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropDuplicates().display()   # 'drop_duplicates()' is also valid"
      ],
      "metadata": {
        "id": "jJFkX1BTLuId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 2**"
      ],
      "metadata": {
        "id": "ldF8wtj4Lmpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping duplicates based on one or more columns\n",
        "\n",
        "df.drop_duplicates(subset = ['Item_Type']).display()"
      ],
      "metadata": {
        "id": "MzCJP3rKLneE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **UNION and UNIONBYNAME**"
      ],
      "metadata": {
        "id": "3Rb5Smt3KVTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = [('1','kad'), ('2','sid')]\n",
        "schema1 = 'id STRING, name STRING'\n",
        "df1 = spark.createDataFrame(data1,schema1)\n",
        "\n",
        "data2 = [('3','rahul'), ('4','jas')]\n",
        "schema2 = 'id STRING, name STRING'\n",
        "df2 = spark.createDataFrame(data2,schema2)"
      ],
      "metadata": {
        "id": "4efoBxRqKa6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.display()"
      ],
      "metadata": {
        "id": "T9kAIhH7Kewn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.display()"
      ],
      "metadata": {
        "id": "LVNcrbhHKgYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.union(df2).display()     # UNION"
      ],
      "metadata": {
        "id": "Bxigpd9ZKhYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNION works even if the ordering of the cols in frame is changed.\n",
        "# ONLY condition is: no. of cols should be same\n",
        "\n",
        "data1 = [('kad','1',), ('sid','2',)]\n",
        "schema1 = 'name STRING, id STRING'\n",
        "\n",
        "df1 = spark.createDataFrame(data1, schema1)\n",
        "df1.display()"
      ],
      "metadata": {
        "id": "j_X5TrCeKj_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.union(df2).display()"
      ],
      "metadata": {
        "id": "Lr4n9gkYKnfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.unionByName(df2).display()     # UNIONBYNAME"
      ],
      "metadata": {
        "id": "vYrjgIUwKn80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **String Functions**"
      ],
      "metadata": {
        "id": "gaiKx2jPKIhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(initcap('Item_Type')).display()  # upper(), lower() fns."
      ],
      "metadata": {
        "id": "e0wQcpDIKJE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(initcap('Item_Type').alias('Initials_Capitalized')).display()"
      ],
      "metadata": {
        "id": "T1bxX2plKJdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **DATE FUNCTIONS**"
      ],
      "metadata": {
        "id": "hJLLj5QCJ3x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('Curr_Date', current_date())    # CURRENT_DATE()\n",
        "df.limit(5).display()"
      ],
      "metadata": {
        "id": "3TdRHENOJ4Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('Week_After', date_add('Curr_Date', 7))   # DATE_ADD()\n",
        "df.limit(5).display()"
      ],
      "metadata": {
        "id": "dM0Rg5DRJ_v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('Week_Before', date_sub('Curr_Date', 7))    # DATE_SUB()\n",
        "df.limit(5).display()"
      ],
      "metadata": {
        "id": "zlp3cS8jKASD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to do the above job\n",
        "\n",
        "df = df.withColumn('Week_Before', date_add('Curr_Date', -7))     #  specifying -7\n",
        "df.limit(5).display()"
      ],
      "metadata": {
        "id": "QWUaT8YbJrCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('DateDiff', datediff('Week_After', 'Curr_Date'))     # DATEDIFF()\n",
        "df.limit(5).display()"
      ],
      "metadata": {
        "id": "GFPWlzi2Jrl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('Week_Before', date_format('Week_Before', 'dd-MM-yyyy'))   # DATE_FORMAT()\n",
        "df.limit(5).display()"
      ],
      "metadata": {
        "id": "Y0QxI9WrJsgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Handling NULLs**"
      ],
      "metadata": {
        "id": "LtU6qKOJJPa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.dropna('all').count()\n",
        "\n",
        "df.dropna().count()  # dropping records having NULL in any col; then, counting remaining records\n",
        "\n",
        "df.dropna(subset=['Outlet_Size']).count()  # dropping records with Outlet_Size = NULL"
      ],
      "metadata": {
        "id": "UA8IOm-uJP4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Filling NA with custom value**"
      ],
      "metadata": {
        "id": "gQkibE8bJIUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace all NULLs with 'NotAvailable' across the frame\n",
        "\n",
        "df.fillna('NotAvailable').limit(10).display()"
      ],
      "metadata": {
        "id": "kn2URLWeJI5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace selected NULLs with 'NotAvailable' in a column; other NULLs remain unaffected\n",
        "\n",
        "df.fillna('NotAvailable', subset = 'Outlet_Size').limit(10).display()"
      ],
      "metadata": {
        "id": "Jv7TteGlJDcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5biW-xFsI-7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **SPLIT** -- Splitting column vals (within the col itself) and accessing them via indexing"
      ],
      "metadata": {
        "id": "cGmBsrv8Itjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('Outlet_Type', split('Outlet_Type', ' ')).limit(5).display()"
      ],
      "metadata": {
        "id": "61bKYHGHIubK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Acessing & displaying the 2nd of the split vals in col 'Outlet_Type'\n",
        "df.withColumn('Outlet_Type', split('Outlet_Type', ' ')[1]).limit(5).display()"
      ],
      "metadata": {
        "id": "6Va11CeXI3Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **EXPLODE** -- multiple vals in a col's cell are exploded over separate rows"
      ],
      "metadata": {
        "id": "ky60t-zxIUP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_explode = df.withColumn('Outlet_Type', split('Outlet_Type', ' '))\n",
        "df_explode.limit(5).display()"
      ],
      "metadata": {
        "id": "oP1AwrIQIU3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_explode.withColumn('Outlet_Type', explode('Outlet_Type')).display()"
      ],
      "metadata": {
        "id": "oMR_eHUUIa98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ARRAY_CONTAINS(col_name, search_string)**  -- returns true/false if col_name contains search_string"
      ],
      "metadata": {
        "id": "1BluyA0fId8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_explode.limit(5).display()"
      ],
      "metadata": {
        "id": "LtcQPmOSIepr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_explode.withColumn('Type1_Flag', array_contains('Outlet_Type', 'Type1')).limit(5).display()"
      ],
      "metadata": {
        "id": "FyAQZUteImQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **groupBy()**"
      ],
      "metadata": {
        "id": "jmNhlTiWHtIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 1**"
      ],
      "metadata": {
        "id": "6zUsUZG9Hug7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.groupBy('Item_Type').agg(sum('Item_MRP')).display()\n",
        "\n"
      ],
      "metadata": {
        "id": "Y1hOgeV1H0jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 2**"
      ],
      "metadata": {
        "id": "MiUDGx2VH4Zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('Item_Type').agg(avg('Item_MRP')).display()\n",
        "\n"
      ],
      "metadata": {
        "id": "Ne95dNjlH6Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 3**"
      ],
      "metadata": {
        "id": "nSkQNOQKH9Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# COMMAND ----------\n",
        "\n",
        "df.groupBy('Item_Type', 'Outlet_Size').agg(sum('Item_MRP').alias('Total_MRP')).display()\n",
        "\n"
      ],
      "metadata": {
        "id": "msM7ABaVH9t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scenario - 4**"
      ],
      "metadata": {
        "id": "BVlhU7t9IA8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('Item_Type', 'Outlet_Size').agg(sum('Item_MRP').alias('Total_MRP'), avg('Item_MRP').alias('Avg_MRP')).display()"
      ],
      "metadata": {
        "id": "W_sZJVjSIBZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ADVANCED TRANSFORMATIONS**"
      ],
      "metadata": {
        "id": "Oz7D0DkRG6D_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **COLLECT_LIST()    -- Same as GROUP_CONCAT() in MySQL**"
      ],
      "metadata": {
        "id": "oWPZ5gTJG71E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('user1','book1'),\n",
        "        ('user1','book2'),\n",
        "        ('user2','book2'),\n",
        "        ('user2','book4'),\n",
        "        ('user3','book1')]\n",
        "\n",
        "schema = 'user STRING, book STRING'\n",
        "df_book = spark.createDataFrame(data, schema)\n",
        "\n",
        "df_book.display()"
      ],
      "metadata": {
        "id": "lWlCVh9GHB9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_book.groupBy('user').agg(collect_list('book')).display()"
      ],
      "metadata": {
        "id": "-dOzPUZmHLT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **PIVOT**"
      ],
      "metadata": {
        "id": "Om7jjBOwHNpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('Item_Type').pivot('Outlet_Size').agg(avg('Item_MRP')).display()"
      ],
      "metadata": {
        "id": "SYg075bzHOaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **WHEN-OTHERWISE**"
      ],
      "metadata": {
        "id": "Q0JCK0CSHVc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('Veg_Flag', when(col('Item_Type') == \"Meat\", \"Non-Veg\").otherwise('Veg'))\n",
        "df.display()"
      ],
      "metadata": {
        "id": "6XBqq8EzHUob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('Veg_Exp_Flag',when(((col('Veg_Flag')=='Veg') & (col('Item_MRP')<100)),'Veg_Inexpensive')\\\n",
        "                            .when((col('Veg_Flag')=='Veg') & (col('Item_MRP')>100),'Veg_Expensive')\\\n",
        "                            .otherwise('Non_Veg')).display()"
      ],
      "metadata": {
        "id": "gM2Ti2lLHYRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **JOINS**"
      ],
      "metadata": {
        "id": "hnmmJJkiAzvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataj1 = [('1','gaur','d01'),\n",
        "          ('2','kit','d02'),\n",
        "          ('3','sam','d03'),\n",
        "          ('4','tim','d03'),\n",
        "          ('5','aman','d05'),\n",
        "          ('6','nad','d06')]\n",
        "schemaj1 = 'emp_id STRING, emp_name STRING, dept_id STRING'\n",
        "df1 = spark.createDataFrame(dataj1,schemaj1)\n",
        "\n",
        "dataj2 = [('d01','HR'),\n",
        "          ('d02','Marketing'),\n",
        "          ('d03','Accounts'),\n",
        "          ('d04','IT'),\n",
        "          ('d05','Finance')]\n",
        "schemaj2 = 'dept_id STRING, department STRING'\n",
        "df2 = spark.createDataFrame(dataj2,schemaj2)"
      ],
      "metadata": {
        "id": "SBzdp5XbA03Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.display()"
      ],
      "metadata": {
        "id": "uTaAWQn7A5r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.display()"
      ],
      "metadata": {
        "id": "oox2m7-qA7N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **INNER, LEFT, RIGHT, ANTI**"
      ],
      "metadata": {
        "id": "NK-wSbFiA_DS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.join(df2, df1['dept_id'] == df2['dept_id'],'inner').display()"
      ],
      "metadata": {
        "id": "x6moqYzMA-b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.join(df2, df1['dept_id'] == df2['dept_id'], 'left').display()"
      ],
      "metadata": {
        "id": "zQixikBIBFr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.join(df2, df1['dept_id'] == df2['dept_id'], 'right').display()"
      ],
      "metadata": {
        "id": "1fcX9UbKBGfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Anti join returns data that is present in one frame BUT NOT in the other frame\n",
        "\n",
        "df1.join(df2, df1['dept_id'] == df2['dept_id'], 'anti').display()"
      ],
      "metadata": {
        "id": "8OYQC3P7BHZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Anti join returns data that is present in one frame BUT NOT in the other frame\n",
        "\n",
        "df2.join(df1, df1['dept_id'] == df2['dept_id'], 'anti').display()"
      ],
      "metadata": {
        "id": "QmQ-1prfBKA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **WINDOW FUNCTIONS**"
      ],
      "metadata": {
        "id": "luXzMffCAlBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "lAIvzQimAoXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('Row_Col', row_number().over(Window.orderBy('Item_Identifier'))).display()"
      ],
      "metadata": {
        "id": "MJbGXLW6Asx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **RANK() and DENSE_RANK()**"
      ],
      "metadata": {
        "id": "w2kmTkdqAe_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('Rank', rank().over(Window.orderBy(col('Item_Identifier').desc())))\\\n",
        "  .withColumn('DenseRank', dense_rank().over(Window.orderBy(col('Item_Identifier').desc()))).display()"
      ],
      "metadata": {
        "id": "KaFlNgADAfsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cumulative Sum**"
      ],
      "metadata": {
        "id": "NLhwORmZAN26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice the CumSum column in the output; it's NOT the desired cumulative sum\n",
        "\n",
        "df.withColumn('CumSum', sum('Item_MRP').over(Window.orderBy('Item_Type'))).display()"
      ],
      "metadata": {
        "id": "NfmTVWccAP_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the correct cumulative sum\n",
        "\n",
        "df.withColumn('CumSum',sum('Item_MRP').over(Window.orderBy('Item_Type').rowsBetween(Window.unboundedPreceding,Window.currentRow))).display()"
      ],
      "metadata": {
        "id": "SUU2V9keAV4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOTAL SUM\n",
        "\n",
        "df.withColumn('TotalSum',sum('Item_MRP').over(Window.orderBy('Item_Type').rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing))).display()"
      ],
      "metadata": {
        "id": "5Symn2TpAW1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **User Defined Functions**"
      ],
      "metadata": {
        "id": "dufKdNRI_Ylu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_func(x):\n",
        "    return x*x\n",
        "\n",
        "my_udf = udf(my_func)   # 'udf' is inbuilt to invoke user defined function\n",
        "\n",
        "df.withColumn('MyNewCol', my_udf('Item_MRP')).display()"
      ],
      "metadata": {
        "id": "DMFKeXQj_cUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **DATA WRITING**"
      ],
      "metadata": {
        "id": "qla-rNqy_i84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format('csv').save('/FileStore/tables/CSV/data.csv')"
      ],
      "metadata": {
        "id": "Nhqq8HYy_k1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Data Writing Modes -- Append, Overwrite, Error, Ignore**"
      ],
      "metadata": {
        "id": "rvXSq6hF_yb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format('csv')\\\n",
        "        .mode('append')\\\n",
        "        .save('/FileStore/tables/CSV/data.csv')\n",
        "\n",
        "df.write.format('csv')\\\n",
        "        .mode('append')\\\n",
        "        .option('path','/FileStore/tables/CSV/data.csv')\\\n",
        "        .save()\n",
        "\n",
        "df.write.format('csv')\\\n",
        "    .mode('overwrite')\\\n",
        "    .option('path','/FileStore/tables/CSV/data.csv')\\\n",
        "    .save()"
      ],
      "metadata": {
        "id": "5qw4x-zR_xCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Error mode (Following code SHOULD throw error)**"
      ],
      "metadata": {
        "id": "I9I2UY_n_KJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format('csv')\\\n",
        "    .mode('error')\\\n",
        "    .option('path','/FileStore/tables/CSV/data.csv')\\\n",
        "    .save()"
      ],
      "metadata": {
        "id": "2T58w0pE_LTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Ignore error mode**"
      ],
      "metadata": {
        "id": "jv8wgFNr_Aom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format('csv')\\\n",
        "  .mode('ignore')\\\n",
        "  .option('path','/FileStore/tables/CSV/data.csv')\\\n",
        "  .save()"
      ],
      "metadata": {
        "id": "1vYVMK6q_BpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **PARQUET format**\n",
        "\n"
      ],
      "metadata": {
        "id": "bk6W3D2f-4Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format('parquet')\\\n",
        "    .mode('overwrite')\\\n",
        "    .option('path','/FileStore/tables/CSV/data.csv')\\\n",
        "    .save()"
      ],
      "metadata": {
        "id": "EqN06PlI-2FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Table format**"
      ],
      "metadata": {
        "id": "p7xyE4rV-lxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format('parquet')\\\n",
        "    .mode('overwrite')\\\n",
        "    .saveAsTable('my_table')"
      ],
      "metadata": {
        "id": "jEh4cZCD-qZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Spark SQL**"
      ],
      "metadata": {
        "id": "8_G0XDko-J3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.display()"
      ],
      "metadata": {
        "id": "XI3ezq_M-L6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createTempView('my_view')  # temporary views are purged as session ends"
      ],
      "metadata": {
        "id": "1xDU54Nu-bQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAGIC %sql\n",
        "select * from my_view"
      ],
      "metadata": {
        "id": "2d15aUMs-GJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAGIC %sql\n",
        "select * from my_view where Item_Fat_Content = 'Low Fat'"
      ],
      "metadata": {
        "id": "DDaSJJQg9_du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql = spark.sql(\"select * from my_view where Item_Fat_Content = 'Low Fat'\")"
      ],
      "metadata": {
        "id": "nIgkkwVQ94UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql.display()"
      ],
      "metadata": {
        "id": "xg2bukiL92xD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}